{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "import pandas as pd\n",
    "import requests_html\n",
    "import os\n",
    "from requests_html import AsyncHTMLSession\n",
    "from requests_html import HTMLSession\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current year to save in applicable folder and for the year_stored column\n",
    "current_year = datetime.date.today().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_employee_to_employees(employees, name, job_title, location):\n",
    "    #get the clean name (in format of \"first_name surname\") without the title\n",
    "    clean_name = get_name_without_title(name, location)\n",
    "    #create the row for the new employee\n",
    "    new_employee = {\"employee_name\" : name, \"first_matched_job_title\" : job_title, \"employee_name_clean\": clean_name, \"location\":location, \"year_stored\":current_year}\n",
    "    #add new employee at the end of the employees data frame\n",
    "    employees.loc[0 if pd.isnull(employees.index.max()) else employees.index.max() + 1] = new_employee\n",
    "    return employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the url for the contact persons based on the location\n",
    "def get_url(location):\n",
    "    if location == \"stuttgart\":\n",
    "        base_url = \"https://www.dhbw-\" + location + \".de\"\n",
    "        url = base_url + \"/dhbw-\" + location + \"/ansprechpersonen/\"\n",
    "    elif location == \"campus-horb\":\n",
    "        base_url = \"https://www.dhbw-\" + \"stuttgart\" + \".de\"\n",
    "        url = base_url + \"/horb/\" + location + \"/ansprechpersonen/\"\n",
    "    elif location == \"loerrach\":\n",
    "        base_url = \"https://www.dhbw-\" + \"loerrach\" + \".de\"\n",
    "        url = base_url + \"/ansprechpersonen/\"\n",
    "    elif location == \"mosbach\":\n",
    "        base_url = \"https://www.\" + location + \".dhbw.de\"\n",
    "        url = base_url + \"/dhbw-\" + location + \"/who-is-who\"\n",
    "    elif location == \"villingen-schwenningen\":\n",
    "        base_url = \"https://www.dhbw-vs.de\"\n",
    "        url = base_url + \"/hochschule/mitarbeitende.html\"\n",
    "    elif location == \"heilbronn\":\n",
    "        base_url = \"https://www.\" + location + \".dhbw.de\"\n",
    "        url = base_url + \"/ueber-uns/team.html\"\n",
    "    else:\n",
    "        base_url = \"https://www.\" + location + \".dhbw.de\"\n",
    "        url = base_url + \"/dhbw-\" + location + \"/ansprechpersonen\"\n",
    "    return url, base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_without_title(name, location):\n",
    "    \n",
    "    employee_name_splitted = name.split(\" \")\n",
    "    employee_name_clean = ''\n",
    "    \n",
    "    for part in employee_name_splitted:\n",
    "        if \".\" in part:\n",
    "            continue\n",
    "        else:\n",
    "            employee_name_clean += part.strip() + \" \"\n",
    "    if( location in [\"stuttgart\", \"campus-horb\", \"loerrach\", \"mosbach\", \"villingen-schwenningen\", \"heilbronn\"]):\n",
    "        return employee_name_clean.replace(\",\",\"\").strip()\n",
    "    else:\n",
    "        employee_name_splitted = employee_name_clean.split(\",\")\n",
    "        employee_name_clean = employee_name_splitted[1].strip() + \" \" + employee_name_splitted[0].strip()\n",
    "        return employee_name_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_title_of_name(employee_name):\n",
    "    if (any( (match := x) in employee_name.lower() for x in name_title_match)):\n",
    "        return match.strip()\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_job_title(job_title):\n",
    "    if (any( (match := x) in job_title.strip().lower() for x in job_title_match)):\n",
    "        return match.strip()\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_others_to_employees(results, employees, location):\n",
    "    for person in results:\n",
    "        #split the text to get the name and the job titles for the person\n",
    "        if( (location in [\"mosbach\", \"villingen-schwenningen\", \"heilbronn\"])):\n",
    "            splitted_person = person.strip().split(\"\\n\")\n",
    "        else:    \n",
    "            splitted_person = person.get_text().strip().split(\"\\n\")\n",
    "        \n",
    "        #skip if the split has only a length of 1 or below. means that there is no job title --> therefore irrelevant\n",
    "        if(len(splitted_person)>1):\n",
    "            job_titles = filter(None, splitted_person[2:])\n",
    "\n",
    "            employee_name = splitted_person[0].strip()\n",
    "            \n",
    "            #get the first matched title for the person\n",
    "            for job_title in job_titles:\n",
    "                title_match = get_matched_job_title(job_title)\n",
    "                if (title_match):\n",
    "                    break\n",
    "\n",
    "            title_match_name = get_matched_title_of_name(employee_name)\n",
    "            \n",
    "            #always prefer the job title over an title match in a name\n",
    "            if (title_match):\n",
    "                employees = add_new_employee_to_employees(employees, employee_name, title_match, location)\n",
    "            elif (title_match_name):\n",
    "                employees = add_new_employee_to_employees(employees, employee_name, title_match_name, location)\n",
    "            \n",
    "    return employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if directory already exists, if not create the directory\n",
    "def check_dir(file_name):\n",
    "    directory = os.path.dirname(file_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### employees for Ravensburg, Mannheim, Heidenheim, Karlsruhe, Campus-Horb, Stuttgart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"ravensburg\", \"mannheim\", \"heidenheim\", \"karlsruhe\", \"loerrach\", \"mosbach\", \"villingen-schwenningen\", \"heilbronn\", \"campus-horb\", \"stuttgart\"]\n",
    "#key words to identify relevant job titles\n",
    "job_title_match = [\"akademisch\", \"professor\", \"studiengangsleiter\", \"wissenschaftlich\", \"studiengangsleitung\", \"prof.*\", \"researcher\"]\n",
    "#key words to identify relevant name titles\n",
    "name_title_match = [\"prof.\", \"dr.\"]\n",
    "\n",
    "#use the async session to retrieve the html data (else it is in a comment section so one can't navigate through it)\n",
    "session = AsyncHTMLSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in locations:\n",
    "    #define the employees data frame where all employees of one location are stored in\n",
    "    employees = pd.DataFrame(data=None, columns=[\"employee_name\", \"first_matched_job_title\", \"employee_name_clean\", \"location\", \"year_stored\"])\n",
    "    \n",
    "    url, base_url = get_url(location)\n",
    "    \n",
    "    r = await session.get(url)\n",
    "    await r.html.arender()\n",
    "    \n",
    "    page_soup = BeautifulSoup(r.html.html, \"html.parser\")\n",
    "    \n",
    "    #for stuttgart and campus-horb the job title is only available on an extra site for the person\n",
    "    if((location == 'stuttgart') | (location == \"campus-horb\")):\n",
    "        results = page_soup.find_all('span', class_=\"name\")\n",
    "        \n",
    "        for result in results:\n",
    "            s = result.find('a', href=True)\n",
    "\n",
    "            if s['href'].startswith(base_url):\n",
    "                person_url = s['href']\n",
    "            else:\n",
    "                person_url = base_url + s['href']    \n",
    "            \n",
    "            r = await session.get(person_url)\n",
    "            await r.html.arender()\n",
    "            soup = BeautifulSoup(r.html.html, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                person_name = soup.find(attrs={\"itemprop\":\"name\"}).string\n",
    "                person_job_title = soup.find(attrs={\"itemprop\":\"jobTitle\"}).string\n",
    "            except:\n",
    "                print(\"exception for url: \" + person_url)\n",
    "                continue\n",
    "\n",
    "            title_match = get_matched_job_title(person_job_title)\n",
    "            title_match_name = get_matched_title_of_name(person_name)\n",
    "        \n",
    "            if (title_match):\n",
    "                employees = add_new_employee_to_employees(employees, person_name, title_match, location)\n",
    "            elif (title_match_name):\n",
    "                employees = add_new_employee_to_employees(employees, person_name, title_match_name, location)\n",
    "        \n",
    "    else:\n",
    "        if((location == 'ravensburg') | (location == 'heidenheim')):\n",
    "            people = page_soup.find_all('a', attrs={\"class\": \"accordion-toggle collapsed\", \"data-parent\":\"#accordion-dhbwcontacts-az\"})\n",
    "        elif(location == 'loerrach'):\n",
    "            people = page_soup.find_all('div', class_=\"panel-title\")\n",
    "        elif(location == 'mosbach'):    \n",
    "            result_names = page_soup.find_all('div', class_=\"card-name\")\n",
    "            result_titles = page_soup.find_all('div', class_=\"card-extra\")\n",
    "            people = [name.get_text() + \"\\n\\n\" + title.get_text(\"\\n\") for name, title in zip(result_names, result_titles)]\n",
    "        elif(location == 'villingen-schwenningen'):\n",
    "            results = page_soup.find_all('div', class_ = \"textcontainer\")\n",
    "            people = [result.get_text().strip() for result in results]\n",
    "        elif(location == 'heilbronn'):\n",
    "            results = page_soup.find_all('div', attrs={\"class\": \"box-0 content-textpic plugin-\"})\n",
    "            people = [result.get_text(\"\\n\").strip() for result in results]\n",
    "        else:\n",
    "            people = page_soup.find_all('a', attrs={\"class\": \"accordion-toggle collapsed\"})\n",
    "        employees = add_others_to_employees(people, employees, location)\n",
    "\n",
    "    file_name = f'../data/{current_year}/employees_{location}.csv'\n",
    "    check_dir(file_name)\n",
    "    employees.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a csv file with all relevant employees of all locations and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021', '2022']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_in_data = os.listdir(\"../data\")\n",
    "\n",
    "p = re.compile('[0-9]{4}')\n",
    "year_list = [ s for s in files_in_data if p.match(s)]\n",
    "year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_employees = pd.DataFrame(data=None, columns=[\"employee_name\", \"first_matched_job_title\", \"employee_name_clean\", \"location\", \"year_stored\"])\n",
    "\n",
    "for year in year_list:\n",
    "    year_path = f'../data/{year}'\n",
    "\n",
    "    all_employee_files = glob.glob(year_path + \"/employees_*.csv\")\n",
    "\n",
    "    for filename in all_employee_files:\n",
    "        employee_df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        all_employees = pd.concat([all_employees, employee_df], ignore_index=True)\n",
    "\n",
    "file_name_all_employees = f'../data/employees_all_sites_all_years.csv'\n",
    "check_dir(file_name_all_employees)\n",
    "all_employees.to_csv(file_name_all_employees, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
